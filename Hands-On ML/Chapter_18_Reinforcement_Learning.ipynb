{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI Gym"
      ],
      "metadata": {
        "id": "4WdQrL5IYjTd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyehAg0FUGrq",
        "outputId": "d1772355-8458-422c-f399-87e624ec4c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q -U gymnasium\n",
        "%pip install swig\n",
        "%pip install -q -U gymnasium[classic_control,box2d,atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')"
      ],
      "metadata": {
        "id": "WQKpKEc9YzZc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs, info = env.reset(seed=42)\n",
        "obs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJHt11_aZKmN",
        "outputId": "010b3e9e-8d2d-4454-f773-70521ca270b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff8jvzjKZYPR",
        "outputId": "689ea212-8e56-4e30-ffb3-07538666b676"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = env.render()\n",
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qclnBsQZbdn",
        "outputId": "90df6ef9-c5dc-4bc6-faaa-7d8110f37554"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 600, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_environment(env, figsize=(5, 4)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    img = env.render()\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    return img\n",
        "\n",
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "4I9NjP2yZqn7",
        "outputId": "055de995-d94e-4a65-eb55-3f41dfd11d32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt7DPMocZjX5",
        "outputId": "0f562379-036a-452f-ea8f-f866755b415e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action = 1\n",
        "obs, reward, done, truncated, info = env.step(action)\n",
        "obs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xry0m0gpZmE-",
        "outputId": "768ed9f1-a1e7-46aa-a9ac-2a93619f7e77"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhG0a6FQaB4y",
        "outputId": "8c55b8b7-08f7-44e0-e6f2-b9359d173373"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02GDCDj6aF3q",
        "outputId": "445ce691-9eb6-429d-a194-da81cc4f74b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "truncated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2sXJsHNaGZb",
        "outputId": "494316fd-df21-43ce-b8c0-7693e4bdfd3b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEGaFV4gaHxw",
        "outputId": "cbd834cf-e780-4aab-c8ef-4409e81f5517"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs, info = env.reset(seed=episode)\n",
        "    for step in range(200):\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "        totals.append(episode_rewards)"
      ],
      "metadata": {
        "id": "2XjQDOSTaIQG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktgo7tUpbt1t",
        "outputId": "58e94474-2b72-4826-918a-582710fee15d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21.713696004717676, 13.151677912691767, 1.0, 62.0)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Policy"
      ],
      "metadata": {
        "id": "wr--jjhxcgIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])"
      ],
      "metadata": {
        "id": "lKYnauXybyox"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Gradient"
      ],
      "metadata": {
        "id": "o9pqLRC4nHtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "        y_target = tf.constant([1.] - tf.cast(action, tf.float32))\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, truncated, info = env.step(int(action))\n",
        "    return obs, reward, done, truncated, grads"
      ],
      "metadata": {
        "id": "n4lmwU_SnJiE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        obs, info = env.reset()\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, truncated, grads = play_one_step(env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done or truncated:\n",
        "                break\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "    return all_rewards, all_grads"
      ],
      "metadata": {
        "id": "-chNcKvJnx5F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the sum of discounted future rewards\n",
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "# normalize\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / reward_std\n",
        "            for discounted_rewards in all_discounted_rewards]"
      ],
      "metadata": {
        "id": "c9Cbt74eon_W"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discount_rewards([10, 0, -50], discount_factor=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj9XKJyypcpU",
        "outputId": "6639f6c4-86d8-4b85-a7d2-02c6337e0bd9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-22, -40, -50])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_factor=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIf8R4iTpjbp",
        "outputId": "9c4c92b2-e177-4f3e-8760-7060e487b6b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
              " array([1.26665318, 1.0727777 ])]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations = 10\n",
        "n_episodes_per_update = 5\n",
        "n_max_steps = 200\n",
        "discount_factor = 0.95"
      ],
      "metadata": {
        "id": "x06Cxa8opqDf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.binary_crossentropy"
      ],
      "metadata": {
        "id": "_wxPfwzxp7AA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iteration in range(n_iterations):\n",
        "    all_rewards, all_grads = play_multiple_episodes(\n",
        "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
        "\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean(\n",
        "            [final_reward * all_grads[episode_index][step][var_index]\n",
        "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "                for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "ZiOUconIqCWx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Markov Decision Process"
      ],
      "metadata": {
        "id": "TVb_ziMTtya2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transition_probabilities = [  # shape=[s, a, s']\n",
        "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
        "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
        "    [None, [0.8, 0.1, 0.1], None]\n",
        "]\n",
        "rewards = [  # shape=[s, a, s']\n",
        "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
        "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
        "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
        "]\n",
        "possible_actions = [[0, 1, 2], [0, 2], [1]]"
      ],
      "metadata": {
        "id": "yBgSN8DSqbRi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values = np.full((3, 3), -np.inf)\n",
        "for state, actions in enumerate(possible_actions):\n",
        "    Q_values[state, actions] = 0.0"
      ],
      "metadata": {
        "id": "nIvZMyktt9Ga"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.90\n",
        "\n",
        "for iteration in range(50):\n",
        "    Q_prev = Q_values.copy()\n",
        "    for s in range(3):\n",
        "        for a in possible_actions[s]:\n",
        "            Q_values[s, a] = np.sum([\n",
        "                transition_probabilities[s][a][sp]\n",
        "                * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n",
        "                for sp in range(3)])"
      ],
      "metadata": {
        "id": "lexf_DYuujlZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kedpIMvhu3Je",
        "outputId": "94139aea-84a0-4938-bc54-78a76129f476"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18.91891892, 17.02702702, 13.62162162],\n",
              "       [ 0.        ,        -inf, -4.87971488],\n",
              "       [       -inf, 50.13365013,        -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(Q_values, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heiBZt5eu6jN",
        "outputId": "1bd1af41-f0ee-46cc-fad4-3ec84e813581"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning"
      ],
      "metadata": {
        "id": "OW3kUNTxwPjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step(state, action):\n",
        "    probas = transition_probabilities[state][action]\n",
        "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
        "    reward = rewards[state][action][next_state]\n",
        "    return next_state, reward"
      ],
      "metadata": {
        "id": "bBbCDDppymbu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exploration_policy(state):\n",
        "    return np.random.choice(possible_actions[state])"
      ],
      "metadata": {
        "id": "QFOcRU46y0K5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha0 = 0.05\n",
        "decay = 0.005\n",
        "gamma = 0.90\n",
        "state = 0\n",
        "\n",
        "for iteration in range(10000):\n",
        "    action = exploration_policy(state)\n",
        "    next_state, reward = step(state, action)\n",
        "    next_value = Q_values[next_state].max()\n",
        "    alpha = alpha0 / (1 + iteration * decay)\n",
        "    Q_values[state, action] *= 1 - alpha\n",
        "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
        "    state = next_state"
      ],
      "metadata": {
        "id": "jxDwrhWNzBA1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q-Learning"
      ],
      "metadata": {
        "id": "NJsvL4De1T4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep-Q-Network\n",
        "input_shape = [4]\n",
        "n_outputs = 2\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='elu', input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(32, activation='elu'),\n",
        "    tf.keras.layers.Dense(n_outputs)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzN13-Pfze7J",
        "outputId": "1de7c8ff-c05c-411a-c26d-ada8805fe246"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# epsilon greedy policy\n",
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        Q_values = model.predict(state[np.newaxis], verbose=0)[0]\n",
        "        return Q_values.argmax()"
      ],
      "metadata": {
        "id": "BVihpF6u1yc6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replay buffer\n",
        "from collections import deque\n",
        "\n",
        "replay_buffer = deque(maxlen=2000)"
      ],
      "metadata": {
        "id": "irFkAxfL2Pfc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly sampling experience from buffer\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
        "    batch = [replay_buffer[index] for index in indices]\n",
        "    return [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(6)\n",
        "    ]   # [states, actions, rewards, next_states, dones, truncateds]"
      ],
      "metadata": {
        "id": "pR-NLM5A2bGj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, truncated, info = env.step(action)\n",
        "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
        "    return next_state, reward, done, truncated, info"
      ],
      "metadata": {
        "id": "B36ZzZME3Ccq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "discount_factor = 0.95\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
        "    next_Q_values = model.predict(next_states, verbose=0)\n",
        "    max_next_Q_values = next_Q_values.max(axis=1)\n",
        "    runs = 1.0 - (dones | truncateds)  # 에피소드가 완료되지 않거나 중단되지 않음\n",
        "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "LSKsgCdk3mjq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "for episode in range(500):\n",
        "    obs, info = env.reset()\n",
        "\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500 , 0.01)\n",
        "        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)"
      ],
      "metadata": {
        "id": "r8qYz3qI4tRI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}